# 8. The Missing Bits

Before attention based neural networks, RNNs were the standard approach for dealing with temporal sequences such as text or sound samples.
- There are also LSTMs
- And GRUs

**Auto-Encoder** - Type of NN that is used for unsupervised learning and dimensionality reduction. It's designed to learn efficient representations of input data by encoding it into a lower-dimensional space and then decoding it backk to it's original form.

**GANs** - Designed to generate new data that is similar to a given training dataset. A generator and a discriminator are trained together in a game-theoric framework.

**Fine-Tuning** - Taking a large pre-trained model (foundational model) and adapting it to a specific task or dataset.

**Graph Neural Networks** - A type of NN architecture designed to process and analyze data structured as graphs. Graphs consist of nodes (verticies) and edges (connections), where each node can represent entities, and edges represent relationships or interactions between entites.

**Self-Supervised Learning** - The model learns from a dataset without relying o explicity human-labeled labels for every day point. Instead, it leverages the inherent structure of patterns within the data itself to create its own supervision signals.
- The principle is to define a task that does not rquire labels but necessitates feature representations which are useful for the reale task of interest.