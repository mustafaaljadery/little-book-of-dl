# Efficient Computation

A GPU processes several thousand parallel units, and its own fast memory. 

The limiting factor is usually not the number of computing units but the read-write operations to memory.

The slowest link is between the CPU memory and GPU memory, and consequently one should avoid copying data across devices.

You want to cache your batches of your samples togther and entirely in the GPU memory and are processed in parallel.

Processing by batches allow for copying the model parameters only once, instead of doing it for each sample.

**FLOPS** - Floating Point Operations Per Second. It's a way to measure how fast a computer or devicee can do math calculations on numbers with decimals.

Deep learning frameworks organize quantities to be processed by organizing them as tensors, which are series of scalars arranged along several discrete aces.

**Activations** - The output value of any layer in a neural network. After a neuron is done calculating, you run through an activation function. 

Tensors are instrumental in achieving computational efficiency.