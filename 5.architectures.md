# Architectures

**Multi-Layer Perceptron** - Takens the form of a succession of fully connected layers separated by activation functions.

**Activation Functions** - Without activation functions, the neural network would behave like a linear model and be limited in its ability to learn and represent intricate patterns in data. 

After each fully connected layer you usually have an activation function like a ReLU.

**Convolutional Networks** - Used for processing images. 

**Batch Norm** - Normalizes the output from the activation function such that all features can be on the same scale. Multiplies normalized output by arbitrary parameter g. Add arbitrary parameter b to resulting product.
- Normalization: Adjusting the values to a standard scale or range to make thm more comparable and manageable.

**Transformer** - A type of neural network architecture that introduced self-attention. Self-attention allows the model to weigh the importance of differnete words or tokkens in a sequence when processing each token. This is amazing for handling long-range dependencies.
- Encoder: Proceesses the input sequence to get a refined representation.
- Decoder: An auto-regressor that generates each token of the result sequence, given the encoder's representation of the input sequence and the output tokens generated so far.

**Cross Attention** - Used to condition a model with some more context information. Focus on different parts of the source sequence while generating each element of the target sequence.

**GPT** - Scales really well with hundreds of billions of trainable parameters.

**Visual Transformer** - A transformer that is designed specifically for processing and understanding visual data, such as images.
