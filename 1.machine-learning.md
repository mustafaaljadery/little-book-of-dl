# 1. Machine Learning

The simpliest way to train is when you have inputs X that map to each output Y. **Supervised Learning**
- By default this is a hard problem because it's hard to come up with an analytical recipe to map X with a huge dimension space. 

**Trainable Parameters** - Coefficients or weights that a machine learning model adjusts during the training process in order to make predictions or classifications. (w)
- This is all of the learning in the model.

**Bias Term** - The bias term adjusts to other factors that my affect the model. 
- Example: Let's say you are trying to predict home price with size as input. The price of a home at place X is more than at place Y even though they are the same size. The bias term (b) accounts for all additional parameters like locations, view, etc.
- It is updated during every run of back prop as well.

- **Loss** - Less the better the function is at predicting the output. You minimize the loss to learn more. 

In addition to weights and biases, models usually depend on meta-parameters.

**Meta-Parameters**
- Learning Rate: The learning rate is the step-size at which the function adjusts the weights and biases.
- Batch Size: Instread of updating the model's parameters after every single training example, which can be computationally expensive, training is done in smaller groups called batches. Larger batch sizes can lead to faster training but they take a lot more memory.
- Hidden Layers: These are the layers that sit between the input layer and the output layer. They are hidden because they don't interact with the outside world.
- Regularization Strength: This is used to avoid overfitting to complex data. It influences the optimization process.
- Epoch: Representation of one complete pass through the entire training set during the training process. Too many epochs and you will have overfitting, too little and you won't have as many.

**Mean Squared Error** - Measures the average squared difference between the predicted values and the actual target values in the training data.

**Capacity of Model** - The ability to represent and learn complex relationships within that data it's trained on. Basically how diverse the model is. You don't want this strong in either direction as it will lead to underfitting/overfitting.

**Underfitting** - The model is to simple to understand the relationships that it produces a lot of errors.

**Overfitting** - The model is to complex that it isn't diverse. It's not general enough for external data, only the dataset you provide it. 

**Inductive Bias** - You choose the right structure of deep learning model be incorporated on the problem you are attacking.

Category of models
- Regression: Best for predicting continuous numeric values. (Supervised)
- Classification: X maps to Y. (Supervised)
- Density Modeling: Estimate and model the probabiliy distribution of a set of data points. Used to detect anomolies and different data points. (Unsupervised)

**Unsupervised Learning** - The function learns without labeled data.
- K-means clustingering: 

**Self-Supervised Learning** - The model generates its own labels from the available data.
