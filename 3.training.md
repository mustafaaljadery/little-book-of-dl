# 3. Training

Training is all about minimizing the loss and optimization for the right set of weights for your function.

The minimization of the loss is key here. Which involves mathematical and computational difficulties.

**Cross Entropy Loss** - A very common loss function that is used in classification probelms that measures the dissimilarity between the predicted probabilities of classes and the actual target labels.

**Soft Max** - Used as activation functions in output layers of a neural network to convert raw scores (logits) into probability distributions.

**Metric Learning** - Type of machine learning that focuses on learning distance metrics or similarity measures between data points.

**Contrastive Learning** - Create a pair of data points and then adjust the embeddings so that the distance between similar pairs is minimized, and the distance between dissimilar pairs is maximized.

Usually, the loss minimized during training is not the actual quantity one wants to optimize but a proxy for which finding the model parameters is easier.

**Weight decay** - Technique in machine learning using to prevent overfitting models. It involves a penalty term to the loss function during training that encourages the model's weights to be smaller.
- This degrades performance on the training set but reduces the gap between teh performance in training and that on new, unseen data.

**Autoregressive model** - A model that tries to predict what happens next based on what happened before. 

**Discrete Sequence** - A series of separate, distinct elements or values that are not continuous. Each element in the sequence is individual or specific.

**Causal Models** - A way to understand and represent cause-and-effect relationships between variables. They help us make sense of how changes in one variable might influence other variables. They try to measure how things are connected.

**Tokenzier** - the conversion to and from the token representation is done by this algorithm.
- Byte Pair Encoding: It's a game of combining letters to make new words. It's a way to compress representations of words into new symbols.

The tool of choice to minimize functions is called gradient descent.

**Gradient Descent** - It's finding the best way down hill. What is going to get me where I want to go the fastest? It's used to optimize the parameters. 
- It improves the estimate of parameters by iterating gradient steps, each consisting of computing the gradient of the loss with respect to the parameters.

**Learning Rate** - It's a positive value that modulates how quickly the minimization is done, and must be chosen carefully.

**Stochastic Gradient Descent** - It's a faster version of gradient descent that uses random jumps to find the best path downhill.
- Imagine you're trying to go down a hill, but instead of taking one careful step at a time, you sometimes take a quick, random jump. These jumps may not be perfect, but they help you get down the hill faster.
- Start with a relatively large learning rate and make it smaller with each step. This is called a schedule.

**Adam Optimizer** - An optimization algorithm useed to update the parameters of the model during training. Start with bigger steps, and then go to smaller ones. Helps models converge to good solutions faster and with less manual tuning of parameters.

**Back Prop** - Teach a model how to improve it's predictions by fine-tuning its internal settings. Compare the model's predictions to the actual answers.

**Auto Grad** - Autograd is an assistant that automatically calculates how changing one thing affects everything else in a complex formula.

Backward passes require twice the memory as a forward pass as you have to save the results of the forward pass to calculate the Jacobian.
- Jacobian: A collections of slops that show how changing multiple things at once affects a bunch of other things.

The memory requirement during inference is roughly equal to that of the most demanding individual layer.

**Reversible Layers** - Layers that can be put together and taken apart without losing any information. Used to retain all the original input information during forward and backward passes.

**Checkpointing** - Saving your progress to continue from that point if you made a mistake.

**Vanishing Gradient** - The gradient becomes smaller and smaller as more layers go through. This is a problem.

**Gradient Norm Clipping** - Technique used during training to control the magnitude of gradients when they get too large.

Empirical evidence accumulated over 20 years demonstrates that state-of-the-art performance across application domains necessitates models with tens of layers.

**Residual Neural Networks** - Type of deep neural network architecture designed to make training very deep networks more effective and efficient. 

**Transformers** - Type of architecture designed for processing sequences of data, particularly natural language.

Increasing the depth leads to a greater complexity of the resulting mapping.

Model size usually makes a better model. Kaplan scaling laws.