# 4. Model Components

A deep modele is nothing but a bunch of computations.

Greater prformance is achieved with deeper architectures, long composition of mappings.

**Layers** - Building blocks that process and transform data as it moves through a neural netowrk. They represent different levels of abstraction and perform specific computations on the input data. 

**Linear Layer** - A layer that performs a linear transformation on the input data. Something like y=x.

**Affine** - Combination of stretching, rotating, and moving things around on a coordinate plane.

**Fully Connected Layers** - They are a type of layer where every neuron is connected to every neuron in the previous layer.

Gradient descent starts with all the parametrs random initialization.

**Convolutional Layers** - Specialized for processing grid-like data, such as images. Convolutional layers are designed to capture local patterns and relationships in the data.
- They can detect patterns in images.
- Filters are reusable patterns or templates that slide over an input image to highlight certain features. These are just a matrix and you can determine the number of rows and columns.
- Convolving -> slide over each matrix in the image. You then calculate the dot product of the filter and the input matrix.
- Filters are commonly 3x3. The intensity of each filter is determined by back prop.

**Dot Product** - How two things align in the same direction. How similar are two vectors (the direction they point).
- a dot b is a1* b1 + a2*b2 + .. + an * bn

**Feature Map** - A visualization of the responses of filters applied to an input image.

**Max Pool** - Select the maximum value in each region. It's a convolution but instead of calculating the dot product, you are calculating the max() of each region.

**Average Pooling** - Select the average/mean value of a region.

**Padding** - Adding another border around an image before applying filters to ensure that important information at the edges isn't lost. At default there isn't a padding.

Convolutions are used to recombine information, generally to reduce the spatial size of the representation, in exchange for a greater number of channels, which translates into a richer local representation.
- Map large-dimensional tensors to low.

The most common used activation function is a ReLU function. It sets negative values to zero and keep positive values unchanged.

Pooling basically summarizes the information.

**Dropout** - You drop some neurons in a layer during each training iteration. This is useful because it prevents overfitting. The network learns to be less senstive to the presence of any specific neuron. This is only during training of course.

**Batch Norm** - Ensure the input to each layer of the network is centered and has a reasonable spread. This helps in faster training and improved convergence of the network.

**Skip Connections** - Connections that allow information to bypass certain layers and be directly passed to subsequent layers.

**Attention Layers** - Used to weigh different paths of input data differently, enabling the model to focus on relevant information.
- Address this problem by computing an attention score.

**Multi-head Attention** - A technique used in attention mechanisms to enhance the model's ability to focus on various aspects of input data. It combines that attention of many heads.

**Self-attention** - Operation that allows a model to weight different parts of input data based on their relationship within the same data.

**Cross-attention** - This enables one part of the sequence to attend to another part.

**Embedding Layer** - Converts categorical data into vectors that can be processed by the model.

**Positional Encoding** - Used to provide information about the order of words in a sequence.

